{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:21.667099Z",
     "start_time": "2019-12-12T21:49:21.063293Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "from time import sleep, time\n",
    "import lxml\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import unicodedata\n",
    "from bs4.element import Comment\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:21.671204Z",
     "start_time": "2019-12-12T21:49:21.669072Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {'illness': [],\n",
    "        'signs': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:22.035725Z",
     "start_time": "2019-12-12T21:49:22.033275Z"
    }
   },
   "outputs": [],
   "source": [
    "search_url = \"https://www.medicinenet.com/diseases_and_conditions/alpha_z.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:22.487338Z",
     "start_time": "2019-12-12T21:49:22.482708Z"
    }
   },
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    clean = re.sub(cleanr, ' ', str(raw_html))\n",
    "    cleaner = clean.strip()\n",
    "    cleantext = re.sub('\\n', ' ', cleaner)\n",
    "    return cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:23.891504Z",
     "start_time": "2019-12-12T21:49:23.879605Z"
    }
   },
   "outputs": [],
   "source": [
    "def export_table(data):\n",
    "    table = pd.DataFrame(data, columns=['illness', 'signs'])\n",
    "    table.index = table.index + 1\n",
    "    table.to_csv('health_data.csv', mode='a', encoding='utf-8', index=False)\n",
    "    \n",
    "    print('Scraping done. Here are the results:')\n",
    "    print(table.info())\n",
    "\n",
    "def sign_details(sign):\n",
    "\n",
    "    r = requests.get(sign)\n",
    "    r.encoding = 'utf-8'\n",
    "    sleep(1)\n",
    "\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        illness = soup.find('h1').text\n",
    "    except:\n",
    "        illness = 'NaN'\n",
    "        \n",
    "    try:\n",
    "        texts = soup.find_all(text=True)\n",
    "        visible_texts = filter(tag_visible, texts)  \n",
    "        signs = u\" \".join(t.strip() for t in visible_texts)\n",
    "    except:\n",
    "        signs = 'NaN'\n",
    "        \n",
    "#     try:\n",
    "#         signs = bs.find('div', class_=\"apPage\").find_all('p')\n",
    "#     except:\n",
    "#         signs = 'NaN'\n",
    "\n",
    "    data['illness'].append(illness)\n",
    "    data['signs'].append(cleanhtml(signs))\n",
    "\n",
    "def extract_title(search_url):\n",
    "\n",
    "    sleep(1)\n",
    "    page = requests.get(search_url)\n",
    "    bs = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        test = bs.find('div', class_=\"AZ_results\").find_all('a')\n",
    "        page_url = [link.get('href') for link in test]\n",
    "    except:\n",
    "        pass\n",
    "   \n",
    "    valid = [] \n",
    "    for val in page_url: \n",
    "        try:\n",
    "            if len(val) > 10: \n",
    "                valid.append(val)\n",
    "        except:\n",
    "            pass \n",
    "    \n",
    "            \n",
    "    \n",
    "    for sign in valid:\n",
    "        sign_details(sign)\n",
    "        \n",
    "    return export_table(data)\n",
    "\n",
    "#     next_page_text = bs.find('div', class_=\"pagination\").find_all('a')\n",
    "#     next_page = [link.get('href') for link in next_page_text][-1]\n",
    "\n",
    "#     if '&start=20' not in next_page:\n",
    "#         next_page_url = (urljoin('https://indeed.com', cleanhtml(next_page)))\n",
    "#         print(next_page_url)\n",
    "#         extract_title(next_page_url)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:28.647482Z",
     "start_time": "2019-12-12T21:49:24.760967Z"
    }
   },
   "outputs": [],
   "source": [
    "extract_title(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-12T21:49:31.328723Z",
     "start_time": "2019-12-12T21:49:31.312455Z"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page = requests.get(url)\n",
    "# bs = BeautifulSoup(page.content, 'html.parser')\n",
    "# for div in bs.find('div', class_=\"apPage\"):\n",
    "#     signs = bs.find_all('p')\n",
    "# signs = cleanhtml(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanhtml(signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = 'https://www.medicinenet.com/zika_virus/article.htm#what_is_zika_virus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_html(body):\n",
    "    r = requests.get(html)\n",
    "    r.encoding = 'utf-8'\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    texts = soup.find_all(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "print(text_from_html(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('health_data.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(data, columns=['illness', 'signs'])\n",
    "table.index = table.index + 1\n",
    "table.to_csv('health_data.csv', mode='a', encoding='utf-8', index=False)\n",
    "    \n",
    "print('Scraping done. Here are the results:')\n",
    "print(table.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = \"https://www.medicinenet.com/amyotrophic_lateral_sclerosis/article.htm\"\n",
    "\n",
    "r = requests.get(sign)\n",
    "r.encoding = 'utf-8'\n",
    "\n",
    "\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "try:\n",
    "    illness = soup.find('h1').text\n",
    "except:\n",
    "    illness = 'NaN'\n",
    "        \n",
    "try:\n",
    "    signs = soup.find('div', class_='wrapper').text\n",
    "except:\n",
    "    signs = 'NaN'\n",
    "\n",
    "data['illness'].append(illness)\n",
    "data['signs'].append(cleanhtml(signs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://www.medicinenet.com/diseases_and_conditions/alpha_a.htm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(search_url)\n",
    "bs = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "test = bs.find('div', class_=\"AZ_results\").find_all('a')\n",
    "page_url = [link.get('href') for link in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid = [] \n",
    "for val in page_url: \n",
    "    if val != None : \n",
    "        valid.append(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
